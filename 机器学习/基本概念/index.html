<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>基本概念 | 黄宝权</title>
    <meta name="generator" content="VuePress 1.8.2">
    <script type="text/javascript" src="/blog/assets/js/push.js"></script>
    <script type="text/javascript" src="/blog/assets/js/hm.js"></script>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC&amp;display=swap">
    <meta name="description" content="">
    <meta name="baidu-site-verification" content="code-FacPXl4enQ">
    
    <link rel="preload" href="/blog/assets/css/0.styles.4ce1ea91.css" as="style"><link rel="preload" href="/blog/assets/js/app.e04d3960.js" as="script"><link rel="preload" href="/blog/assets/js/2.1cd0f1d8.js" as="script"><link rel="preload" href="/blog/assets/js/39.d803664c.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.83705009.js"><link rel="prefetch" href="/blog/assets/js/11.012cc240.js"><link rel="prefetch" href="/blog/assets/js/12.1713f7ef.js"><link rel="prefetch" href="/blog/assets/js/13.10c6016c.js"><link rel="prefetch" href="/blog/assets/js/14.62c27229.js"><link rel="prefetch" href="/blog/assets/js/15.343047bf.js"><link rel="prefetch" href="/blog/assets/js/16.092845b7.js"><link rel="prefetch" href="/blog/assets/js/17.20ee64bc.js"><link rel="prefetch" href="/blog/assets/js/18.1fbb094a.js"><link rel="prefetch" href="/blog/assets/js/19.50ce8b88.js"><link rel="prefetch" href="/blog/assets/js/20.53e3dead.js"><link rel="prefetch" href="/blog/assets/js/21.72c1232d.js"><link rel="prefetch" href="/blog/assets/js/22.249a9fb1.js"><link rel="prefetch" href="/blog/assets/js/23.075349db.js"><link rel="prefetch" href="/blog/assets/js/24.0421a1e0.js"><link rel="prefetch" href="/blog/assets/js/25.1d843f38.js"><link rel="prefetch" href="/blog/assets/js/26.bfc48946.js"><link rel="prefetch" href="/blog/assets/js/27.18c19e9c.js"><link rel="prefetch" href="/blog/assets/js/28.9163e697.js"><link rel="prefetch" href="/blog/assets/js/29.9b48fa32.js"><link rel="prefetch" href="/blog/assets/js/3.09e2c500.js"><link rel="prefetch" href="/blog/assets/js/30.9479eb7b.js"><link rel="prefetch" href="/blog/assets/js/31.5d62a607.js"><link rel="prefetch" href="/blog/assets/js/32.51de4f21.js"><link rel="prefetch" href="/blog/assets/js/33.b6c93f3e.js"><link rel="prefetch" href="/blog/assets/js/34.ff97fe4e.js"><link rel="prefetch" href="/blog/assets/js/35.13ac9b06.js"><link rel="prefetch" href="/blog/assets/js/36.671aa168.js"><link rel="prefetch" href="/blog/assets/js/37.922e0e0f.js"><link rel="prefetch" href="/blog/assets/js/38.d896a1a7.js"><link rel="prefetch" href="/blog/assets/js/4.b00891c3.js"><link rel="prefetch" href="/blog/assets/js/40.16902c2f.js"><link rel="prefetch" href="/blog/assets/js/41.053cb64d.js"><link rel="prefetch" href="/blog/assets/js/42.43dbe754.js"><link rel="prefetch" href="/blog/assets/js/43.b31322b9.js"><link rel="prefetch" href="/blog/assets/js/44.33f0cc7a.js"><link rel="prefetch" href="/blog/assets/js/45.2f461bbe.js"><link rel="prefetch" href="/blog/assets/js/46.e82a9c05.js"><link rel="prefetch" href="/blog/assets/js/47.02a0dcfd.js"><link rel="prefetch" href="/blog/assets/js/48.aeca67d7.js"><link rel="prefetch" href="/blog/assets/js/49.aa5edcc9.js"><link rel="prefetch" href="/blog/assets/js/5.eed5e89c.js"><link rel="prefetch" href="/blog/assets/js/50.4addb3f5.js"><link rel="prefetch" href="/blog/assets/js/51.4c393fba.js"><link rel="prefetch" href="/blog/assets/js/52.e61f3e25.js"><link rel="prefetch" href="/blog/assets/js/6.eea7144c.js"><link rel="prefetch" href="/blog/assets/js/7.336b4090.js"><link rel="prefetch" href="/blog/assets/js/8.a0b65074.js"><link rel="prefetch" href="/blog/assets/js/9.9f5d9c91.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.4ce1ea91.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div data-v-af323418><header class="header" data-v-69452f20 data-v-af323418><div class="title" data-v-69452f20>黄宝权</div></header> <nav class="navs" data-v-38628940 data-v-af323418></nav> <div class="search-box" data-v-10b1314e data-v-af323418><input aria-label="Search" autocomplete="off" spellcheck="false" value="" data-v-10b1314e> <!----></div> <div class="tags" data-v-1272fba8 data-v-af323418><div class="tag" data-v-1272fba8><a data-v-1272fba8>Git</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>Java</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>JavaScript</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>Linux</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>MySQL</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>Node.js</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>Web</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>操作系统</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>数据分析</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>数据库</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>数据结构</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>算法</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>爬虫</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>机器学习</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>编程语言</a></div><div class="tag" data-v-1272fba8><a data-v-1272fba8>网络</a></div></div> <main data-v-af323418><div data-v-e4554dac data-v-af323418><!----> <div class="content__default" data-v-e4554dac><h1 id="基本概念"><a href="#基本概念" class="header-anchor">#</a> 基本概念</h1> <h2 id="机器学习-ml-machine-learning"><a href="#机器学习-ml-machine-learning" class="header-anchor">#</a> 机器学习（ML, Machine Learning）</h2> <p>让机器学习一些数据然后建立模型的过程。</p> <ul><li>回归：预测一些数值，比如预测股价、经济增长速度等（监督学习）。</li> <li>分类：对有标签的数据进行分类（监督学习）。</li> <li>聚类：对无标签的数据进行分类（无监督学习）。</li> <li>降维：用低维的模型表示高维的数据，保留基本属性。</li></ul> <h3 id="模型"><a href="#模型" class="header-anchor">#</a> 模型</h3> <p>模型就是输入输出的映射，可以理解为是一个函数或一个方法或一个公式。（比如，y = wx + b）</p> <h3 id="特征-feature"><a href="#特征-feature" class="header-anchor">#</a> 特征（Feature）</h3> <p>对应模型输入的 x，有多少特征就有多少输入（x1, x2, x3, ...），比如芒果的颜色、大小、重量等。</p> <h3 id="标签-label"><a href="#标签-label" class="header-anchor">#</a> 标签（Label）</h3> <p>对应模型输出的 y，比如芒果的成熟程度。</p> <h3 id="参数"><a href="#参数" class="header-anchor">#</a> 参数</h3> <p>对应模型的 w 和 b。</p> <h3 id="监督学习"><a href="#监督学习" class="header-anchor">#</a> 监督学习</h3> <p>对有标签的数据进行学习（告诉模型输入的 x 和对应输出的 y）。</p> <h3 id="无监督学习"><a href="#无监督学习" class="header-anchor">#</a> 无监督学习</h3> <p>对无标签的数据进行学习（只告诉模型输入的 x，不告诉输出的 y）。</p> <h3 id="半监督学习"><a href="#半监督学习" class="header-anchor">#</a> 半监督学习</h3> <p>对少量有标签和大量无标签的数据进行学习（告诉模型一部分数据输入的 x 和对应输出的 y）。</p> <h3 id="强化学习"><a href="#强化学习" class="header-anchor">#</a> 强化学习</h3> <p>对无标签的数据进行学习，然后对符合正确预期的学习进行奖励从而强化。</p> <h3 id="深度学习"><a href="#深度学习" class="header-anchor">#</a> 深度学习</h3> <p>使用多层神经网络模型进行学习。</p> <h3 id="损失函数-误差函数-lost-function"><a href="#损失函数-误差函数-lost-function" class="header-anchor">#</a> 损失函数 / 误差函数（Lost Function）</h3> <p>用于评价模型的好坏，定义在单个样本上，算的是一个样本的误差。</p> <ul><li>0 - 1 损失函数（预测正确，损失函数为 0；预测错误，损失函数为 1。）</li> <li>平方损失函数（预测值与真实值差的平方）</li> <li>绝对损失函数（预测值与真实值差的绝对值）</li> <li>对数损失函数</li></ul> <h3 id="代价函数-成本函数-cost-function"><a href="#代价函数-成本函数-cost-function" class="header-anchor">#</a> 代价函数 / 成本函数（Cost Function）</h3> <p>用于在平均意义上评价模型的好坏，定义在所有样本上，算的是所有样本误差和的平均，代价函数 = 损失函数和的平均（期望）。</p> <ul><li>均方误差（MSE, Mean Squared Error）：平方损失函数和的平均，一般用于线性回归模型。</li> <li>均方根误差（RMSE, Root Mean Squared Error）：均方误差的算术平方根。</li> <li>平均绝对误差（MAE, Mean Absolute Error）：绝对损失函数和的平均。</li> <li>交叉熵（Cross Entry）：一般用于逻辑回归模型。</li></ul> <h3 id="目标函数-object-function"><a href="#目标函数-object-function" class="header-anchor">#</a> 目标函数（Object Function）</h3> <p>用于评价模型的好坏，目标函数 = 代价函数（经验风险） + 正则化项（结构风险）。</p> <blockquote><p>很多时候，损失函数、代价函数、目标函数往往都不进行区分，都混为一谈。</p></blockquote> <h3 id="风险函数-risk-function"><a href="#风险函数-risk-function" class="header-anchor">#</a> 风险函数（Risk Function）</h3> <ul><li>经验风险：决定模型的拟合程度，拟合程度越高，经验风险越小（样本容量不大的情况下，容易产生过拟合问题）。<strong>与代价函数有关。</strong></li> <li>结构风险：决定模型的复杂程度，模型结构越简单，结构风险越小。<strong>与正则化项（也叫做惩罚项）有关。</strong></li></ul> <h3 id="正则化"><a href="#正则化" class="header-anchor">#</a> 正则化</h3> <p>即结构风险最小化。</p> <h3 id="过拟合"><a href="#过拟合" class="header-anchor">#</a> 过拟合</h3> <p>对数据拟合程度过高（经验风险小，结构风险大）。</p> <h3 id="欠拟合"><a href="#欠拟合" class="header-anchor">#</a> 欠拟合</h3> <p>对数据拟合程度过差（经验风险大，结构风险小）。</p> <h3 id="优化器-optimizer"><a href="#优化器-optimizer" class="header-anchor">#</a> 优化器（Optimizer）</h3> <p>解决机器学习中最优化（最优解）的问题，求解函数极值，包括极大值和极小值。</p> <p>比如，可用来优化损失（代价）函数，求解损失（代价）函数最小损失（代价）的解。</p> <ul><li>梯度下降法（GD, Gradient Descent）：在有限视距内寻找最快的路径下山，因此每走一步，都要参考当前位置最陡的方向才迈出下一步。准确度较高，速度慢，并且容易陷入局部最优解。</li> <li>随机梯度下降法（SGD, Stochastic Gradient Descent）：下山的时候随便乱下，虽然下山过程会显得扭扭曲曲，但是总能下到山底。准确度较低，速度快，并不是全局最优，不易于并行实现。</li> <li>批量梯度下降法（BGD, Batch Gradient Descent）：在下山之前就掌握了整座山的地势情况，选择总体平均梯度最小的方向下山。准确度高，可得到全局最优解，易于并行实现，当样本的数目很多时，速度会变慢。</li> <li>小批量梯度下降法（MBGD, Mini-Batch Gradient Descent）：在下山之前掌握了一部分的地势情况，选择总体平均梯度最小的方向下。准确度比 BGD 算法低，速度比 BGD 算法快。</li> <li>动量法（Momentum）：在下山的时候带上了加速度，下山越来越快。速度会因为加速度变得更快，准确度较低，因为加速度导致刹不住脚从而容易错过山底跑到另一个山坡上。</li> <li>牛顿加速梯度法（NAG, Nesterov Accelerated Gradient）：在下山的时候带上了加速度，同时时刻观察是否快到山底，然后快到山底的时候把速度降下来。速度比 Momentum 慢，准确度比 Momentum 高。</li></ul> <h3 id="梯度"><a href="#梯度" class="header-anchor">#</a> 梯度</h3> <p>函数在某一点处的梯度是这样的一个向量（矢量），它的方向是最大方向导数的方向，它的值是最大方向导数的值。</p> <h3 id="学习率"><a href="#学习率" class="header-anchor">#</a> 学习率</h3> <p>与优化器有关，决定学习速度，学习率（加速度）越大，则速度越快，同时也会更加容易错过极值点。</p> <h3 id="二分类"><a href="#二分类" class="header-anchor">#</a> 二分类</h3> <p>给定一个样本作为输入，输出的结果只能有两个（Yes/No、1/0、是/否）。</p> <p>比如是否是垃圾邮件、是否是肿瘤、是否是癌症等。</p> <h3 id="激活函数-activation-function"><a href="#激活函数-activation-function" class="header-anchor">#</a> 激活函数 （Activation Function）</h3> <p>是神经网络模型中的神经元，它接受上层神经元的输出，其实也就成为了它的输入，把它的输入进行求和后，通过它的激活函数计算出结果，再输出给下层神经元，这样神经网络模型的表达能力就更加强大。</p> <ul><li>Sigmoid</li> <li>Softmax</li> <li>Tanh</li> <li>ReLU</li> <li>PReLU</li> <li>ELU</li></ul> <h3 id="广义线性模型"><a href="#广义线性模型" class="header-anchor">#</a> 广义线性模型</h3> <p>研究变量之间的关系。</p> <h4 id="线性回归-监督学习"><a href="#线性回归-监督学习" class="header-anchor">#</a> 线性回归（监督学习）</h4> <p>用于对连续值的预测，比如身高和体重的关系，根据身高预测体重等。</p> <p>模型：f(x) = wx + b</p> <h4 id="非线性回归-监督学习"><a href="#非线性回归-监督学习" class="header-anchor">#</a> 非线性回归（监督学习）</h4> <p>非线性回归模型与线性回归模型原理基本一样，区别在于线性回归模型是直线，非线性回归模型是曲线。</p> <p>模型：f(x) = w1x + w2x^2 + ... + b</p> <h4 id="逻辑回归-监督学习"><a href="#逻辑回归-监督学习" class="header-anchor">#</a> 逻辑回归（监督学习）</h4> <p>用于对离散值的预测，比如二分类问题、多分类问题等。</p> <p>模型：</p> <ul><li>Sigmoid：可用来解决二分类问题。</li> <li>Softmax：可用来解决二分类、多分类问题。</li> <li>K 近邻（KNN, K-Nearest Neighbor）：可用来解决二分类、多分类问题。</li></ul> <h3 id="神经网络模型-nn-neural-network-人工神经网络模型-ann-artificial-neural-network"><a href="#神经网络模型-nn-neural-network-人工神经网络模型-ann-artificial-neural-network" class="header-anchor">#</a> 神经网络模型（NN, Neural Network）/ 人工神经网络模型（ANN, Artificial Neural Network）</h3> <p>具备多层结构，1 个输入层（无计算逻辑，直接传递数据），0 个或多个隐含层（有计算逻辑，计算层），1 个输出层（有计算逻辑，计算层），每层由 1 个或多个神经元组成，而一个神经元就是一个函数（激活函数）。</p> <p>输入层没有计算逻辑，是因为输入层的神经元没有激活函数，或者说它的激活函数是线性函数 f(x) = x。</p> <p>输入层的神经元个数与特征数一致，输出层的神经元个数与标签数一致，隐含层的神经元个数根据设计者的经验决定。</p> <blockquote><p>因为神经网络模型是根据线性模型发展出来的，所以也可以用来解决线性回归的问题，实现 f(x) = wx + b。</p> <p>实现方法：用 1 个输入层加上 1 个输出层，每层 1 个神经元，并且把输出层的激活函数去掉。</p></blockquote> <h4 id="单层神经网络-监督学习"><a href="#单层神经网络-监督学习" class="header-anchor">#</a> 单层神经网络（监督学习）</h4> <p>1 个输入层，0 个隐含层，1 个输出层。</p> <h4 id="两层神经网络-监督学习"><a href="#两层神经网络-监督学习" class="header-anchor">#</a> 两层神经网络（监督学习）</h4> <p>1 个输入层，1 个隐含层，1 个输出层。</p> <h4 id="多层神经网络-深度学习-三层或三层以上"><a href="#多层神经网络-深度学习-三层或三层以上" class="header-anchor">#</a> 多层神经网络（深度学习）（三层或三层以上）</h4> <p>1 个输入层，2+ 个隐含层，1 个 输出层。</p> <ul><li>深度神经网络（DNN, Deep Neural Network）（监督学习）：神经元全连接，容易造成梯度消失。</li> <li>卷积神经网络（CNN, Convolutional Neural Networks）（监督学习）：适合空间矩阵数据，比如图像识别、人脸识别等。</li> <li>循环神经网络（RNN, Recurrent Neural Network）（监督学习）：适合时间序列数据，比如语音识别、文本翻译、股价预测等。</li></ul> <h3 id="生成式对抗网络模型-gan-generative-adversarial-networks-深度学习-无监督学习"><a href="#生成式对抗网络模型-gan-generative-adversarial-networks-深度学习-无监督学习" class="header-anchor">#</a> 生成式对抗网络模型（GAN, Generative Adversarial Networks）（深度学习 / 无监督学习）</h3> <p>生成器和鉴别器的互相博弈学习产生相当好的输出。</p> <p>比如，可用来生成高仿真的以假乱真的数据（图片、语音、视频、文字等）。</p></div></div></main> <footer data-v-81a5c08c data-v-af323418></footer></div><div class="global-ui"><!----></div></div>
    <script src="/blog/assets/js/app.e04d3960.js" defer></script><script src="/blog/assets/js/2.1cd0f1d8.js" defer></script><script src="/blog/assets/js/39.d803664c.js" defer></script>
  </body>
</html>
