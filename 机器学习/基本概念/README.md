# 基本概念

## 机器学习（ML, Machine Learning）

让机器学习一些数据然后建立模型的过程。

- 回归：预测一些数值，比如预测股价、经济增长速度等（监督学习）。
- 分类：对有标签的数据进行分类（监督学习）。
- 聚类：对无标签的数据进行分类（无监督学习）。
- 降维：用低维的模型表示高维的数据，保留基本属性。

### 模型

模型就是输入输出的映射，可以理解为是一个函数或一个方法或一个公式。（比如，y = wx + b）

### 特征（Feature）

对应模型输入的 x，有多少特征就有多少输入（x1, x2, x3, ...），比如芒果的颜色、大小、重量等。

### 标签（Label）

对应模型输出的 y，比如芒果的成熟程度。

### 参数

对应模型的 w 和 b。

### 监督学习

对有标签的数据进行学习（告诉模型输入的 x 和对应输出的 y）。

### 无监督学习

对无标签的数据进行学习（只告诉模型输入的 x，不告诉输出的 y）。

### 半监督学习

对少量有标签和大量无标签的数据进行学习（告诉模型一部分数据输入的 x 和对应输出的 y）。

### 强化学习

对无标签的数据进行学习，然后对符合正确预期的学习进行奖励从而强化。

### 深度学习

使用多层神经网络模型进行学习。

### 损失函数 / 误差函数（Lost Function）

用于评价模型的好坏，定义在单个样本上，算的是一个样本的误差。

- 0 - 1 损失函数（预测正确，损失函数为 0；预测错误，损失函数为 1。）
- 平方损失函数（预测值与真实值差的平方）
- 绝对损失函数（预测值与真实值差的绝对值）
- 对数损失函数

### 代价函数 / 成本函数（Cost Function）

用于在平均意义上评价模型的好坏，定义在所有样本上，算的是所有样本误差和的平均，代价函数 = 损失函数和的平均（期望）。

- 均方误差（MSE, Mean Squared Error）：平方损失函数和的平均，一般用于线性回归模型。
- 均方根误差（RMSE, Root Mean Squared Error）：均方误差的算术平方根。
- 平均绝对误差（MAE, Mean Absolute Error）：绝对损失函数和的平均。
- 交叉熵（Cross Entry）：一般用于逻辑回归模型。

### 目标函数（Object Function）

用于评价模型的好坏，目标函数 = 代价函数（经验风险） + 正则化项（结构风险）。

> 很多时候，损失函数、代价函数、目标函数往往都不进行区分，都混为一谈。

### 风险函数（Risk Function）

- 经验风险：决定模型的拟合程度，拟合程度越高，经验风险越小（样本容量不大的情况下，容易产生过拟合问题）。**与代价函数有关。**
- 结构风险：决定模型的复杂程度，模型结构越简单，结构风险越小。**与正则化项（也叫做惩罚项）有关。**

### 正则化

即结构风险最小化。

### 过拟合

对数据拟合程度过高（经验风险小，结构风险大）。

### 欠拟合

对数据拟合程度过差（经验风险大，结构风险小）。

### 优化器（Optimizer）

解决机器学习中最优化（最优解）的问题，求解函数极值，包括极大值和极小值。

比如，可用来优化损失（代价）函数，求解损失（代价）函数最小损失（代价）的解。

- 梯度下降法（GD, Gradient Descent）：在有限视距内寻找最快的路径下山，因此每走一步，都要参考当前位置最陡的方向才迈出下一步。准确度较高，速度慢，并且容易陷入局部最优解。
- 随机梯度下降法（SGD, Stochastic Gradient Descent）：下山的时候随便乱下，虽然下山过程会显得扭扭曲曲，但是总能下到山底。准确度较低，速度快，并不是全局最优，不易于并行实现。
- 批量梯度下降法（BGD, Batch Gradient Descent）：在下山之前就掌握了整座山的地势情况，选择总体平均梯度最小的方向下山。准确度高，可得到全局最优解，易于并行实现，当样本的数目很多时，速度会变慢。
- 小批量梯度下降法（MBGD, Mini-Batch Gradient Descent）：在下山之前掌握了一部分的地势情况，选择总体平均梯度最小的方向下。准确度比 BGD 算法低，速度比 BGD 算法快。
- 动量法（Momentum）：在下山的时候带上了加速度，下山越来越快。速度会因为加速度变得更快，准确度较低，因为加速度导致刹不住脚从而容易错过山底跑到另一个山坡上。
- 牛顿加速梯度法（NAG, Nesterov Accelerated Gradient）：在下山的时候带上了加速度，同时时刻观察是否快到山底，然后快到山底的时候把速度降下来。速度比 Momentum 慢，准确度比 Momentum 高。

### 梯度

函数在某一点处的梯度是这样的一个向量（矢量），它的方向是最大方向导数的方向，它的值是最大方向导数的值。

### 学习率

与优化器有关，决定学习速度，学习率（加速度）越大，则速度越快，同时也会更加容易错过极值点。

### 二分类

给定一个样本作为输入，输出的结果只能有两个（Yes/No、1/0、是/否）。

比如是否是垃圾邮件、是否是肿瘤、是否是癌症等。

### 激活函数 （Activation Function）

是神经网络模型中的神经元，它接受上层神经元的输出，其实也就成为了它的输入，把它的输入进行求和后，通过它的激活函数计算出结果，再输出给下层神经元，这样神经网络模型的表达能力就更加强大。

- Sigmoid
- Softmax
- Tanh
- ReLU
- PReLU
- ELU

### 广义线性模型

研究变量之间的关系。

#### 线性回归（监督学习）

用于对连续值的预测，比如身高和体重的关系，根据身高预测体重等。

模型：f(x) = wx + b

#### 非线性回归（监督学习）

非线性回归模型与线性回归模型原理基本一样，区别在于线性回归模型是直线，非线性回归模型是曲线。

模型：f(x) = w1x + w2x^2 + ... + b

#### 逻辑回归（监督学习）

用于对离散值的预测，比如二分类问题、多分类问题等。

模型：

- Sigmoid：可用来解决二分类问题。
- Softmax：可用来解决二分类、多分类问题。
- K 近邻（KNN, K-Nearest Neighbor）：可用来解决二分类、多分类问题。

### 神经网络模型（NN, Neural Network）/ 人工神经网络模型（ANN, Artificial Neural Network）

具备多层结构，1 个输入层（无计算逻辑，直接传递数据），0 个或多个隐含层（有计算逻辑，计算层），1 个输出层（有计算逻辑，计算层），每层由 1 个或多个神经元组成，而一个神经元就是一个函数（激活函数）。

输入层没有计算逻辑，是因为输入层的神经元没有激活函数，或者说它的激活函数是线性函数 f(x) = x。

输入层的神经元个数与特征数一致，输出层的神经元个数与标签数一致，隐含层的神经元个数根据设计者的经验决定。

> 因为神经网络模型是根据线性模型发展出来的，所以也可以用来解决线性回归的问题，实现 f(x) = wx + b。
>
> 实现方法：用 1 个输入层加上 1 个输出层，每层 1 个神经元，并且把输出层的激活函数去掉。

#### 单层神经网络（监督学习）

1 个输入层，0 个隐含层，1 个输出层。

#### 两层神经网络（监督学习）

1 个输入层，1 个隐含层，1 个输出层。

#### 多层神经网络（深度学习）（三层或三层以上）

1 个输入层，2+ 个隐含层，1 个 输出层。

- 深度神经网络（DNN, Deep Neural Network）（监督学习）：神经元全连接，容易造成梯度消失。
- 卷积神经网络（CNN, Convolutional Neural Networks）（监督学习）：适合空间矩阵数据，比如图像识别、人脸识别等。
- 循环神经网络（RNN, Recurrent Neural Network）（监督学习）：适合时间序列数据，比如语音识别、文本翻译、股价预测等。

### 生成式对抗网络模型（GAN, Generative Adversarial Networks）（深度学习 / 无监督学习）

生成器和鉴别器的互相博弈学习产生相当好的输出。

比如，可用来生成高仿真的以假乱真的数据（图片、语音、视频、文字等）。
